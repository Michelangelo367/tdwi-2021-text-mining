{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Reddit Autos Selfposts<div class=\"tocSkip\">\n",
    "    \n",
    "&copy; Jens Albrecht, 2021\n",
    "    \n",
    "This notebook can be freely copied and modified.  \n",
    "Attribution, however, is highly appreciated.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: \n",
    "\n",
    "Albrecht, Ramachandran, Winkler: **Blueprints for Text Analytics in Python** (O'Reilly 2020)  \n",
    "Chapter 4: [Preparing Data for Statistics and Machine Learning](https://learning.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html#ch-preparation) + [Link to Github](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:43.150071Z",
     "start_time": "2021-06-17T16:12:43.122460Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/jsalbr/tdwi-2021-text-mining/raw/main'\n",
    "    os.system(f'wget {GIT_ROOT}/notebooks/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:45.588913Z",
     "start_time": "2021-06-17T16:12:43.152273Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/notebooks/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append('./packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.142910Z",
     "start_time": "2021-06-17T16:12:45.591410Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE_DIR}/data/reddit-autos-selfposts-raw.csv\", sep=\";\", decimal=\".\", parse_dates=['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.769976Z",
     "start_time": "2021-06-17T16:12:46.145448Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subreddit'].value_counts().head(20).plot(kind='barh', height=500).update_yaxes(autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.798865Z",
     "start_time": "2021-06-17T16:12:46.771612Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.853721Z",
     "start_time": "2021-06-17T16:12:46.800643Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.952581Z",
     "start_time": "2021-06-17T16:12:46.855433Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.975160Z",
     "start_time": "2021-06-17T16:12:46.955756Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:46.997422Z",
     "start_time": "2021-06-17T16:12:46.977261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Noise with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:47.021136Z",
     "start_time": "2021-06-17T16:12:46.999223Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:47.200548Z",
     "start_time": "2021-06-17T16:12:47.023208Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "# add new column to data frame\n",
    "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
    "\n",
    "# get the top 3 records\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:47.222094Z",
     "start_time": "2021-06-17T16:12:47.202126Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) \n",
    "    text = html.unescape(text) \n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:47.248108Z",
     "start_time": "2021-06-17T16:12:47.224371Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print(\"Impurity:\", impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:49.237690Z",
     "start_time": "2021-06-17T16:12:47.250299Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].progress_map(clean)\n",
    "df['impurity']   = df['clean_text'].apply(impurity, min_len=20)\n",
    "\n",
    "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Normalization with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:49.268274Z",
     "start_time": "2021-06-17T16:12:49.240078Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:49.701685Z",
     "start_time": "2021-06-17T16:12:49.270333Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalize(text):\n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text\n",
    "\n",
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:52.692090Z",
     "start_time": "2021-06-17T16:12:49.703925Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].progress_map(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern-based Data Masking with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:53.482657Z",
     "start_time": "2021-06-17T16:12:52.694751Z"
    }
   },
   "outputs": [],
   "source": [
    "from textacy.preprocessing.resources import RE_URL\n",
    "from blueprints.exploration import count_words\n",
    "\n",
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:53.517636Z",
     "start_time": "2021-06-17T16:12:53.484846Z"
    }
   },
   "outputs": [],
   "source": [
    "from textacy.preprocessing.replace import urls as replace_urls\n",
    "\n",
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "\n",
    "# using default substitution _URL_\n",
    "print(replace_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:56.847057Z",
     "start_time": "2021-06-17T16:12:53.521265Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].progress_map(replace_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Save the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:56.912261Z",
     "start_time": "2021-06-17T16:12:56.849876Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['title'] + ': ' + df['clean_text']\n",
    "\n",
    "df[['subreddit', 'text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:56.950037Z",
     "start_time": "2021-06-17T16:12:56.914659Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['clean_text', 'impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:57.930133Z",
     "start_time": "2021-06-17T16:12:56.955223Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"reddit-autos-selfposts-cleaned.csv\", sep=\";\", decimal=\".\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Processing with spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.514768Z",
     "start_time": "2021-06-17T16:12:57.932660Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.550138Z",
     "start_time": "2021-06-17T16:12:58.517043Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.589330Z",
     "start_time": "2021-06-17T16:12:58.552427Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.628193Z",
     "start_time": "2021-06-17T16:12:58.592243Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.683054Z",
     "start_time": "2021-06-17T16:12:58.630316Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.preparation import display_nlp\n",
    "\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Lemmas based on Part-of-Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.718003Z",
     "start_time": "2021-06-17T16:12:58.684492Z"
    }
   },
   "outputs": [],
   "source": [
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.756335Z",
     "start_time": "2021-06-17T16:12:58.720194Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, include_pos=None, exclude_pos=[]):\n",
    "    return [t.lemma_ \n",
    "            for t in doc \n",
    "            if (include_pos==None or t.pos_ in include_pos) and t.pos_ not in exclude_pos]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['NOUN', 'PROPN'])\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Named Entities\n",
    "\n",
    "### Model-based Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.796293Z",
     "start_time": "2021-06-17T16:12:58.758487Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.839087Z",
     "start_time": "2021-06-17T16:12:58.798578Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:58.873838Z",
     "start_time": "2021-06-17T16:12:58.841537Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None):\n",
    "\n",
    "    return [t.text for t in doc if t.ent_type_ in include_types]\n",
    "\n",
    "print(extract_entities(doc, ['ORG', 'PERSON']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:59.374350Z",
     "start_time": "2021-06-17T16:12:58.876222Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:59.431687Z",
     "start_time": "2021-06-17T16:12:59.376181Z"
    }
   },
   "outputs": [],
   "source": [
    "import cars\n",
    "\n",
    "cars.brands[:5]\n",
    "cars.models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a relaxed pattern - favors high recall but will result in false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:12:59.466965Z",
     "start_time": "2021-06-17T16:12:59.433843Z"
    }
   },
   "outputs": [],
   "source": [
    "patterns = [{\"label\": \"BRAND\", \n",
    "             \"pattern\": [{\"LOWER\": {\"IN\": cars.brands}, \n",
    "                          \"POS\": {\"IN\": [\"PROPN\", \"NOUN\", \"ADJ\"] }\n",
    "                         }]},\n",
    "            {\"label\": \"MODEL\", \n",
    "             \"pattern\": [{\"LOWER\": {\"IN\": cars.models}, \n",
    "                          \"POS\": {\"IN\": [\"PROPN\", \"NOUN\", \"ADJ\"] }\n",
    "                         }]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.022084Z",
     "start_time": "2021-06-17T16:12:59.469895Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.065809Z",
     "start_time": "2021-06-17T16:13:00.024043Z"
    }
   },
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.119815Z",
     "start_time": "2021-06-17T16:13:00.068280Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.157126Z",
     "start_time": "2021-06-17T16:13:00.123218Z"
    }
   },
   "outputs": [],
   "source": [
    "'spark' in cars.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.207755Z",
     "start_time": "2021-06-17T16:13:00.159200Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The spark plugs in my Z4 are making trouble.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.248705Z",
     "start_time": "2021-06-17T16:13:00.209697Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"I currently have a 2018 honda civic that I'd be getting rid of\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.299878Z",
     "start_time": "2021-06-17T16:13:00.251738Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"I hate Mercedes A-Class\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.355218Z",
     "start_time": "2021-06-17T16:13:00.302411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"I hate A Mercedes\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.419323Z",
     "start_time": "2021-06-17T16:13:00.358244Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Is a 2017 Tesla Model S worth it in 2021?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: One Function to Get It All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.459521Z",
     "start_time": "2021-06-17T16:13:00.421963Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas' : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM']),\n",
    "    'nouns'  : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'brands' : extract_entities(doc, ['BRAND']),\n",
    "    'models' : extract_entities(doc, ['MODEL'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.949954Z",
     "start_time": "2021-06-17T16:13:00.461674Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "\n",
    "_ = nlp.add_pipe(\"merge_entities\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "[pipe[0] for pipe in nlp.pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:00.995682Z",
     "start_time": "2021-06-17T16:13:00.951988Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for col, values in extract_nlp(doc).items():\n",
    "    print(f\"{col}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.036953Z",
     "start_time": "2021-06-17T16:13:00.997569Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using spaCy on a Large Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.709730Z",
     "start_time": "2021-06-17T16:13:01.039821Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE_DIR}/data/reddit-autos-selfposts-cleaned.csv\", sep=\";\", decimal=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.756358Z",
     "start_time": "2021-06-17T16:13:01.712516Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.814942Z",
     "start_time": "2021-06-17T16:13:01.759672Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "df[['subreddit', 'text']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.868946Z",
     "start_time": "2021-06-17T16:13:01.818247Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Colab**: Choose \"Runtime\"&rarr;\"Change Runtime Type\"&rarr;\"GPU\" to benefit from the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:01.918328Z",
     "start_time": "2021-06-17T16:13:01.872103Z"
    }
   },
   "outputs": [],
   "source": [
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:02.660530Z",
     "start_time": "2021-06-17T16:13:01.921563Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "\n",
    "_ = nlp.add_pipe(\"merge_entities\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:13:02.701491Z",
     "start_time": "2021-06-17T16:13:02.663020Z"
    }
   },
   "outputs": [],
   "source": [
    "# full data set takes about 5 minutes\n",
    "# for faster processing use a sample like this\n",
    "# df = df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:55.828005Z",
     "start_time": "2021-06-17T16:13:02.708020Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:55.912158Z",
     "start_time": "2021-06-17T16:19:55.829949Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['text', 'lemmas', 'brands', 'models']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing and Saving the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Quick Frequency Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:56.524508Z",
     "start_time": "2021-06-17T16:19:55.915062Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'nouns')\n",
    "\n",
    "freq_df.head(20).plot(kind='barh', height=500).update_yaxes(autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:57.180413Z",
     "start_time": "2021-06-17T16:19:56.527082Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.exploration import wordcloud\n",
    "\n",
    "wordcloud(freq_df['freq'], max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:58.145129Z",
     "start_time": "2021-06-17T16:19:57.184698Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'models')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:58.751815Z",
     "start_time": "2021-06-17T16:19:58.148705Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'brands')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:58.794733Z",
     "start_time": "2021-06-17T16:19:58.755217Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:58.946029Z",
     "start_time": "2021-06-17T16:19:58.796880Z"
    }
   },
   "outputs": [],
   "source": [
    "synonyms = { brand: brand for brand in cars.brands }\n",
    "synonyms['mercedes-menz'] = 'mercedes'\n",
    "synonyms['volkswagen'] = 'vw'\n",
    "\n",
    "df['brands'] = df['brands'].progress_map(lambda brands: [synonyms[b.lower()] for b in brands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:19:59.472184Z",
     "start_time": "2021-06-17T16:19:58.948586Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'brands')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Prepared Data\n",
    "\n",
    "Alternatively into a SQL database or JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:20:01.551059Z",
     "start_time": "2021-06-17T16:19:59.475280Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert lists of tokens into space-separated strings for csv-saving\n",
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items).lower())\n",
    "\n",
    "df.to_csv(f\"reddit-autos-selfposts-prepared.csv\", sep=\";\", decimal=\".\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T16:20:01.609951Z",
     "start_time": "2021-06-17T16:20:01.553693Z"
    }
   },
   "outputs": [],
   "source": [
    "# restore lists\n",
    "# df[nlp_columns] = df[nlp_columns].applymap(lambda items: items.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.627px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
